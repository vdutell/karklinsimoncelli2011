{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dependencies\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils.plotutils as plu\n",
    "import utils.imreadin as imr\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "nneurons = 100\n",
    "patchsize = 128\n",
    "batchsize = 100\n",
    "iterations = 100000\n",
    "testiterations = 10000\n",
    "\n",
    "#noise - these Prateek is interested in changing to removed iid asumption\n",
    "noisexsigma = 0.8\n",
    "noisersigma = 2\n",
    "\n",
    "#to keep covariance matrix small for now\n",
    "npatchestotal =  iterations*batchsize #np.int(1e6)\n",
    "\n",
    "lambdaj = 0.001 #this is adjusted in order to get ravg = 1\n",
    "ravg = 1\n",
    "#minact = 1e-1\n",
    "#bslfr = 1e-1\n",
    "\n",
    "learning_rate = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#full images\n",
    "#vhims = extract_images('../vanHaterenNaturalImages/VanHaterenNaturalImagesCurated.h5')\n",
    "\n",
    "#image patches (as in Karklin& Simoncelli)\n",
    "#only load in image patches if we haven't yet.\n",
    "try:\n",
    "    vhims\n",
    "except NameError:\n",
    "    print(\"Loading Van Hateren Natural Image Database...\")\n",
    "    vhims = imr.vanHateren(\n",
    "        img_dir='../vanHaterenNaturalImages/VanHaterenNaturalImagesCurated.h5',\n",
    "        normalize = True,\n",
    "        patch_edge_size=patchsize\n",
    "        )\n",
    "    print(\"Done Loading!\")    \n",
    "    np.random.shuffle(vhims.images)\n",
    "    print(\"Done Shuffling!\")\n",
    "    \n",
    "    ##reduce images so we only hold in memory the ones we're using\n",
    "    vhims.images = vhims.images[0:npatchestotal]\n",
    "    \n",
    "print(\"Images Loaded. Carry On!\")\n",
    "\n",
    "#code to save if i had enough hd space. i shold buy a new computer.\n",
    "#f=open('../vanHaterenNaturalImages/vhnormpatches_{}.npy'.format(patchsize),'wb')\n",
    "#np.save(f,vhims)\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#example image\n",
    "plt.imshow(vhims.images[400,:,:],cmap='gray', interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "filename ='../vanHaterenNaturalImages/VanHaterenNaturalImagesCurated.h5'\n",
    "normalize = True,\n",
    "patch_edge_size=128\n",
    "normalize=False\n",
    "\n",
    "with h5py.File(filename, \"r\") as f:\n",
    "    full_img_data = np.array(f['van_hateren_good'], dtype=np.float32)\n",
    "    if(normalize):\n",
    "        print('normalizing...')\n",
    "        full_img_data = full_img_data - np.mean(full_img_data,axis=(1,2),keepdims=True)\n",
    "        full_img_data = full_img_data/np.std(full_img_data,axis=(1,2),keepdims=True)\n",
    "    if patch_edge_size is not None:\n",
    "        print('sectioning into patches....')\n",
    "        print(full_img_data.shape)\n",
    "        (num_img, num_px_rows, num_px_cols) = full_img_data.shape\n",
    "        num_img_px = num_px_rows * num_px_cols\n",
    "        #print(num_px_rows)\n",
    "        #print(patch_edge_size)\n",
    "        #print(num_px_rows % patch_edge_size)\n",
    "        #make sure tiling is compatible with dimensions of image\n",
    "        assert (num_px_rows % patch_edge_size == 0)  , (\"The number of image row edge pixels % the patch edge size must be 0.\")\n",
    "        assert (num_px_cols % patch_edge_size == 0)  , (\"The number of image column edge pixels % the patch edge size must be 0.\")\n",
    "        num_patches = int(num_img_px / patch_edge_size**2)\n",
    "        #full_img_data = np.reshape(full_img_data, (num_img, num_img_px)\n",
    "        #data = np.vstack([full_img_data[idx,...].reshape(num_patches, patch_edge_size, patch_edge_size) for idx in range(num_img)])\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        data = full_img_data\n",
    "        num_patches = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.asarray(np.split(full_img_data, num_px_cols/patch_edge_size,2)) # tile column-wise\n",
    "data = np.asarray(np.split(data, num_px_rows/patch_edge_size,2)) #tile row-wise\n",
    "data = np.reshape(np.transpose(data,(3,4,0,1,2)),(patch_edge_size,patch_edge_size,-1)) #stack tiles together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.shape(test)\n",
    "plt.imshow(test[:,:,1000],cmap='gray', interpolation='None')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#covariance parameters\n",
    "\n",
    "#global curvature of images\n",
    "try:\n",
    "    cx\n",
    "except NameError:\n",
    "    print(\"Couldn't find C_x matrix. Calculating now....\")\n",
    "    xr = vhims.images[0:npatchestotal,:,:].reshape(-1,patchsize**2).T.astype('float32')\n",
    "    cx = np.cov(xr)\n",
    "    cxnorm = cx-np.mean(cx) # a normalized version\n",
    "print(\"C_x calculated. Shape is {}, calculated from {} patches.\".format(np.shape(cx),npatchestotal))\n",
    "\n",
    "#cx = np.cov(xr)\n",
    "#cx = np.dot(xr,xr.T)/nimagestest - np.dot(np.mean(xr,axis=1,keepdims=True).T,np.mean(xr,axis=1,keepdims=True))\n",
    "cxnorm= np.eye(patchsize**2)\n",
    "#print(cx)\n",
    "#print(np.linalg.det(cx))\n",
    "#plt.pcolormesh(np.linalg.inv(cx))\n",
    "plt.pcolormesh(cxnorm)\n",
    "plt.colorbar()\n",
    "\n",
    "#noise covariances\n",
    "cnx = np.eye(patchsize**2) * noisexsigma**2\n",
    "cnr = np.eye(nneurons) * noisersigma**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#input image\n",
    "x = tf.placeholder(tf.float32, shape=(patchsize**2,batchsize),name='input_image')\n",
    "\n",
    "#noise\n",
    "nx = tf.placeholder(tf.float32, shape=(patchsize**2,batchsize),name='input_noise')\n",
    "nr = tf.placeholder(tf.float32, shape=(nneurons,batchsize),name='response_noise')\n",
    "\n",
    "#covariances\n",
    "cxt = tf.constant(cxnorm,dtype=tf.float32)#+0.01\n",
    "cnxt = tf.constant(cnx,dtype=tf.float32)#+0.01\n",
    "cnrt = tf.constant(cnr,dtype=tf.float32)#+0.01\n",
    "\n",
    "#weights\n",
    "w = tf.Variable(tf.random_normal(shape=(patchsize**2,nneurons),dtype=tf.float32,stddev=1),name='weights')\n",
    "\n",
    "#relu params - both positive\n",
    "actslope = tf.Variable(np.abs(tf.random_normal([nneurons,1],stddev=1,dtype=tf.float32)),dtype=tf.float32,name='relu_slope')\n",
    "actoffset = tf.Variable(np.abs(tf.random_normal([nneurons,1],stddev=1,dtype=tf.float32)),dtype=tf.float32,name='relu_offset')\n",
    "\n",
    "#activation function\n",
    "def af(slope,offset,y):\n",
    "    #return(1/(1+tf.exp(-1*slope*(y+offset)))) #sigmoid\n",
    "    return(tf.mul(tf.abs(slope),tf.nn.relu(tf.add(y,offset)))) #relu\n",
    "\n",
    "def af_ddy(slope,offset,y):\n",
    "    #return(slope*af(slope,offset,y)*(1-af(slope,offset,y))) #sigmoid\n",
    "    return(tf.mul(tf.cast(tf.less(offset, y),tf.float32),tf.abs(slope))) #relu\n",
    "    \n",
    "#calculate response\n",
    "y = tf.transpose(tf.matmul(tf.transpose(x + nx),w))\n",
    "r = tf.add(af(actslope,actoffset,y), nr)\n",
    "rate = tf.reduce_mean(r,1)\n",
    "meanrate = tf.reduce_mean(rate)\n",
    "\n",
    "#objective function: maximize I(X,R)-sum_j(lambda*<r_j>)\n",
    "#1s for all places respose (y + offset) is greater than zero\n",
    "#mask = tf.cast(tf.less(tf.zeros((nneurons,1)),y+reluoff),tf.float32)\n",
    "#multiply response by reluslope everywhere response was greater than zero, add baseline to this.\n",
    "g = tf.diag(tf.reshape(tf.reduce_mean(af_ddy(actslope,actoffset,y),axis=1),[nneurons]))\n",
    "\n",
    "cxr = tf.matrix_inverse(tf.matrix_inverse(cxt) + #with inverse calculated by Shariq\n",
    "#cxr =  1 * (tf.matrix_inverse(cxt) + #as in paper\n",
    "                        tf.matmul(w, \n",
    "                        tf.matmul(g, \n",
    "                        tf.matmul(tf.matrix_inverse(\n",
    "                        tf.matmul(g,\n",
    "                        tf.matmul(tf.transpose(w), \n",
    "                        tf.matmul(cnxt, \n",
    "                        tf.matmul(w, g)))) + cnrt),\n",
    "                        tf.matmul(g, tf.transpose(w))))))\n",
    "\n",
    "\n",
    "cxr = tf.div(cxr,tf.reduce_max(cxr)) #scale to make determinant smaller\n",
    "\n",
    "#informion = tf.reduce_mean(0.5*tf.log1p(2*np.pi*np.exp(1)*(5)))\n",
    "det_g = tf.matrix_determinant(g)\n",
    "detcxrt = tf.matrix_determinant(cxr)\n",
    "information = 0.5*tf.log1p(2*np.pi*np.exp(1)*(tf.matrix_determinant(cxr)))\n",
    "#information = 0.5*tf.log1p(2*np.pi*np.exp(1)*tf.trace(cxr))\n",
    "objective =  information - lambdaj * tf.reduce_sum(rate)#/nneurons\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(-objective)\n",
    "\n",
    "#run it!\n",
    "#sess = tf.Session\n",
    "with tf.Session() as sess:\n",
    "    #initialize vars\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    #save evolution of system over training\n",
    "    information_evolution = []\n",
    "    objective_evolution = []\n",
    "    rate_evolution = []\n",
    "    detcxrs_evolution = []\n",
    "    \n",
    "    hist = {}\n",
    "    \n",
    "    #train over niterations\n",
    "    nits = iterations #full run\n",
    "    nits = testiterations #test run\n",
    "    \n",
    "    print('Training {} iterations...'.format(nits))\n",
    "    for ii in range(nits):\n",
    "        if(ii%(int(nits/10))==0):\n",
    "            print(str(ii)+', ',end=\"\")\n",
    "        #image = np.random.rand(patchsize**2,batchsize).astype(np.float32) #random image\n",
    "        image = np.reshape(vhims.images[ii*batchsize:(1+ii)*batchsize,:,:],(batchsize,patchsize**2)).T.astype(np.float32)\n",
    "        noise_input = np.random.normal(0,noisexsigma,[patchsize**2,batchsize]).astype(np.float32)\n",
    "        #noise_input = np.zeros([patchsize**2,1]).astype(np.float32)\n",
    "        noise_response = np.random.normal(0,noisersigma,[nneurons,batchsize]).astype(np.float32)\n",
    "        #noise_response = np.zeros([nneurons,1]).astype(np.float32)\n",
    "        hist[0] = image\n",
    "        hist[1] = noise_input\n",
    "        hist[2] = noise_response\n",
    "        \n",
    "        #train\n",
    "        #train_step.run(feed_dict={x:image, nx: noise_input, nr:noise_response})\n",
    "        sess.run(train_step, feed_dict={x:image, nx: noise_input, nr:noise_response})\n",
    "        \n",
    "        #save evolution of params\n",
    "        info = sess.run(information, feed_dict={x:image, nx: noise_input, nr:noise_response})\n",
    "        information_evolution.append(info)\n",
    "        cost = sess.run(objective, feed_dict={x:image, nx: noise_input, nr:noise_response})\n",
    "        objective_evolution.append(cost)\n",
    "        rate = sess.run(meanrate, feed_dict={x:image, nx: noise_input, nr:noise_response})\n",
    "        rate_evolution.append(rate)\n",
    "        detcxr = sess.run(detcxrt, feed_dict={x:image, nx: noise_input, nr:noise_response})\n",
    "        detcxrs_evolution.append(detcxr)\n",
    "#       if ii%iterations == 0:\n",
    "#            pass\n",
    "        \n",
    "    #print statements\n",
    "    #myweights = w.eval()\n",
    "    #print(myweights)\n",
    "    #myrlus = reluslope.eval()\n",
    "    #print(myrlus)\n",
    "    #myrluof = reluoff.e`val()\n",
    "    #print(myrluof)\n",
    "    #myrs = r.eval()\n",
    "    #print(myrs)\n",
    "    #myobjective = objective.eval()\n",
    "    #print(myobjective)\n",
    "    weights = sess.run(w, feed_dict={x:image, nx: noise_input, nr:noise_response})\n",
    "    detz = sess.run(g, feed_dict={x:image, nx: noise_input, nr:noise_response})\n",
    "    print('Done!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,10))\n",
    "plt.subplot(2,2,1,title='Information')\n",
    "plt.plot(information_evolution)\n",
    "\n",
    "plt.subplot(2,2,2,title='Firing Rate')\n",
    "plt.plot(rate_evolution)\n",
    "\n",
    "plt.subplot(2,2,3,title='Objective')\n",
    "plt.plot(objective_evolution)\n",
    "\n",
    "plt.subplot(2,2,4,title='cxrs Determinant')\n",
    "plt.plot(detcxrs_evolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wr = np.rollaxis(np.reshape(weights,(patchsize,patchsize,nneurons)),2)\n",
    "plu.display_data_tiled(wr, normalize=False, title=\"weights\", prev_fig=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.reshape(weights[:,6],(patchsize,patchsize)),interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### weights\n",
    "w = np.random.randn(patchsize**2,nneurons)\n",
    "#relu params - both positive\n",
    "actslope = np.abs(np.random.randn(nneurons,1))\n",
    "actoffset = np.abs(np.random.randn(nneurons,1))\n",
    "\n",
    "\n",
    "def sigmoid_af(slope,offset,y):\n",
    "    #return(1/(1+tf.exp(-1*slope*(y+offset))))\n",
    "    return(np.abs(slope)*((y-offset)>0))\n",
    "\n",
    "def sigmoid_af_ddy(slope,offset,y):\n",
    "    #return(slope*sigmoid_af(slope,offset,y)*(1-sigmoid_af(slope,offset,y)))\n",
    "    mask = (offset[:,0] < y.T)\n",
    "    return(mask*np.abs(slope[:,0]))\n",
    "\n",
    "\n",
    "\n",
    "#calculate response\n",
    "y = np.transpose(np.dot(np.transpose(hist[0][:,0] + hist[1][:,0]),w))\n",
    "#print(np.transpose(hist[0] + hist[1]).shape)\n",
    "#print(w.shape)\n",
    "#print(y.shape)\n",
    "r = np.add(sigmoid_af(actslope, actoffset, y), hist[2][:,0])\n",
    "rate = np.mean(r,0)\n",
    "#print(rate)\n",
    "\n",
    "#g = np.diag(np.reshape(sigmoid_af_ddy(actslope,actoffset,y),-1)+0.1) \n",
    "g = np.diag(np.reshape(sigmoid_af_ddy(actslope,actoffset,y),[nneurons]))\n",
    "\n",
    "print('g')\n",
    "print(g)\n",
    "print(np.linalg.det(g))\n",
    "print(np.trace(g))\n",
    "\n",
    "print('cxnorm')\n",
    "print(cxnorm)\n",
    "print(np.linalg.det(cxnorm))\n",
    "\n",
    "print('cnx')\n",
    "print(cnx)\n",
    "print(np.linalg.det(cnx))\n",
    "\n",
    "print('cnr')\n",
    "print(cnr)\n",
    "print(np.linalg.det(cnr))\n",
    "\n",
    "\n",
    "cxr = np.linalg.inv(np.linalg.inv(cx) + #with inverse calculated by Shariq\n",
    "#cxr =  1 * (tf.matrix_inverse(cx) + #as in paper\n",
    "                        np.dot(w, \n",
    "                        np.dot(g, \n",
    "                        np.dot(np.linalg.inv(\n",
    "                        np.dot(g,\n",
    "                        np.dot(np.transpose(w), \n",
    "                        np.dot(cnx, \n",
    "                        np.dot(w, g)))) + cnr),\n",
    "                        np.dot(g, np.transpose(w))))))\n",
    "\n",
    "print('cxr')\n",
    "print(np.linalg.det(cxr))\n",
    "print(cxr)\n",
    "\n",
    "print('gwcwg')\n",
    "pgwcwg = np.linalg.inv(np.dot(g,np.dot(w.T,np.dot(cnx,np.dot(w,g))))+cnr)\n",
    "gwcwg = np.dot(w,np.dot(g,np.dot(pgwcwg,np.dot(g,w.T))))+np.linalg.inv(cx)\n",
    "\n",
    "print(gwcwg)\n",
    "\n",
    "print(np.linalg.det(gwcwg))\n",
    "\n",
    "plt.pcolormesh(gwcwg)\n",
    "plt.colorbar()\n",
    "\n",
    "information = np.mean(0.5*np.log1p(2*np.pi*np.exp(1)*(np.linalg.det(cxr))))\n",
    "objective =  information - np.sum(lambdaj * rate)#/nneurons\n",
    "\n",
    "print(information)\n",
    "#train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(-objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(cx-np.mean(cx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
